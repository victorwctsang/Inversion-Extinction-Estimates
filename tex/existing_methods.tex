
We explore five existing estimators of extinction times, discussing each approach's assumptions and reasoning to develop a baseline understanding of the methods available in the literature.

\section{Maximum Likelihood Estimator (MLE)}\label{section:MLE}
We begin by describing maximum likelihood estimation as a method for inferring the extinction time of a species. Under the \hyperref[model: no-measurement-error]{$\delta$-model}, assuming uniformity and no measurement error, the Maximum Likelihood Estimator (MLE) is the first order statistic $X_{(1)}$:
\begin{equation}\label{eq:mle}
    \hat\theta_{\text{MLE}} = X_{(1)}
\end{equation}

From a statistical perspective, the MLE procedure is an obvious choice to estimate the extinction time, as one can intuit the most recent fossil as representing the most information about a species' extinction date. However, the incompleteness of the fossil record means that the first and last fossil will never precisely represent the introduction and extinction times of a species. Perhaps for this reason (along with its unrealistic assumptions) maximum likelihood sees limited use in the literature \cite{Ludwig1996, Jaric2016}. Nonetheless, the MLE serves as a straightforward baseline for estimating extinction times.

Consider the likelihood function of the uniform distribution
\[
\mathcal{L}(\theta | \bm{x}) =  \frac{n}{(K - \theta)^{n+1}} > 0 \quad \forall \theta \in \R
\]

where $K$ (the upper bound for $X_i$) is a known constant. Differentiating the likelihood, we can show that the derivative is a monotonically increasing function of $\theta$:
\[
\frac{d}{d\theta}\mathcal{L}(\theta | \bm{x} ) = \begin{cases}
    \frac{1}{(K-\theta)^n} & \text{if $X_{(1)} \geq \theta$ and $X_{(n)} \leq K$} \\
    0 & \text{otherwise}
\end{cases}
\]

where $X_{(i)}$ is the $i$\textsuperscript{th} order statistic.

Thus, our maximum likelihood estimator $\hat\theta_{\text{MLE}}$ is the sample minimum $x_{(1)}$. However, this estimator is positively biased as the most recent fossil is always at least as old as the species' extinction date (for proof, see \autoref{apx:mle-bias-proof}):
\[
    \E[\hat\theta_{\text{MLE}}] = \E[X_{(1)}] = \frac{K}{n+1} + \frac{n}{n+1}\theta
\]

Correcting for this gives us a bias-adjusted MLE:
\begin{equation}\label{eq:ba-mle}
    \hat\theta_{\text{BA-MLE}} = X_{(1)} \frac{n+1}{n} - \frac{K}{n}
\end{equation}

\section{Strauss Estimator}

\citet{Strauss1989} proposed an unbiased estimator based on the maximum likelihood procedure under a slightly modified version of the \hyperref[model: no-measurement-error]{$\delta$-model}, where $K$ is also unknown.

From \autoref{section:MLE}, the MLE for $\theta$ is the first order statistic $X_{(1)}$, and by symmetry the MLE for $K$ is the $n$\textsuperscript{th} order statistic:
\begin{align*}
    \hat\theta_{\text{MLE}} &= X_{(1)} \\
    \hat K_{\text{MLE}} &= X_{(n)}
\end{align*}
Next, we take the expectation of both estimators (see \autoref{apx:strauss-estimator-proof} for proof):
\begin{align*}
    \E \left[ \hat\theta_{\text{MLE}} \right] &= \frac{K + n\theta}{n+1} \\
    \E \left[ \hat K_{\text{MLE}} \right]  &= \frac{nK + \theta}{n+1}
\end{align*}
Solving the equations simultaneously, $K$ can be eliminated to yields the following:
\[
\E \left( \frac{nX_{(1)} - X_{(n)}}{n-1} \right) = \theta
\]
which is then used to derive the following unbiased Strauss estimator as an alternative to the previously found unbiased MLE:
\begin{equation}\label{eq:strauss}
\hat\theta_{\text{Strauss}} = \frac{n X_{(1)} - X_{(n)}}{n-1}
\end{equation}

The key feature distinguishing the Strauss estimator from the MLE is that it does not require assuming a known upper bound $K$. The Strauss estimator is a fairly widely known and well established method, as it has been shown to be optimal under the (unrealistic) assumptions of uniformity and no measurement error \cite{wang_chudzicki_everson_2009}.

\section{McInerny Estimator}

\citet{Mcinerny2006} developed a method for inferring a species' extinction based on previous sightings under the \hyperref[model: no-measurement-error]{$\delta$-model}. By assuming uniformity and no measurement error with sighting rate denoted by $\lambda$, the authors proposed a method of estimating the probability $p$ of a fossil\footnote{Note that the original paper was concerned with estimating the extinction of species that have been seen relatively recently, but their methods can be similarly applied to a paleontology context where a species is almost certainly already extinct. For the remainder of this section, we will refer to fossils, rather than sightings.} being found as a function of $\lambda$ and the time gap since the last observation:
\[
p = \left( 1 - \lambda \right)^{x_1 - \theta} \quad;\quad \lambda = \frac{n}{K - x_1}
\]

where the recovery rate $\lambda$ is estimated as the number of samples between $K$ and $x_1$ divided by the time interval spanned by the observations. To find the extinction time, consider the extinction time $\theta$ as the terminal record --- that is, the date at which the probability of another fossil being found is less than some threshold probability $q$ -- the McInerny estimator $\hat\theta_{\text{MI}}$ is the largest value of $\theta$ that satisfies this condition:
\begin{equation}\label{eqn:mcinerny-estimator}
    \hat\theta_{n; \text{MI}} = \max\left\{ \theta ; \left( 1 - \frac{n}{K - x_1} \right)^{x_1 - \theta} < q \right\}
\end{equation}

There are a number of issues with this method for estimating extinctions in the fossil record. First, the authors' approach to modeling $\lambda$ using the interval $[x_1, K]$ positively biases the estimate, as the period of interest should be bounded by $\theta$ instead of $x_1$. This was noted in \citet{Huang2019}, who suggested the following correction: \[ \lambda = \frac{n}{K - \theta} \] This results in the following corrected McInerny estimator: \begin{equation}\label{eqn:mcinerny-bias-corrected-estimator}
    \hat\theta_{n; \text{MI corrected}} = \max\left\{ \theta ; \left( 1 - \frac{n}{K - \theta} \right)^{x_1 - \theta} < q \right\}
\end{equation}
Since this method can no longer be rearranged to find $\theta$ with respect to constants, a root finding algorithm must be used to find the solution.

Secondly, the selection of the threshold probability $q$ is questionable. \citet{Mcinerny2006} proposed setting $q = 0.05$ corresponding to a 5\% significance level for a hypothesis test. However, this can result in overestimating how recently a species went extinct, as estimates are based on the 5\textsuperscript{th} percentile. To correct for this, we propose setting $q = 0.5$ instead for a median-based estimate.

Finally, a known property of stationary Poisson processes is that the gaps between observations should be exponentially distributed; however, the results from their paper show a geometric distribution with rate $\lambda$ instead. This is because \citet{Mcinerny2006} implicitly assume that time is discretised to whole-numbered values, when measured in years. This approximation is not really necessary, but we expect it to have negligible effect in practice because the time scales of interest are typically in thousands of years.

\section{GRIWM Estimator}

The Gaussian-Resampled Inverse-Weighted McInerny (GRIWM) estimator is an approach based on the previous \citet{Mcinerny2006} method, designed to account for both the influence of sighting rate and radiometric dating error. \citet{Bradshaw2012} assume uniformly distributed fossils and Gaussian-distributed measurement errors for their procedure, which has two main ideas: one, use Gaussian resampling to account for the known radiometric uncertainty associated with each fossil; and two, use the McInerny method to estimate the true extinction time, inversely weighting the contribution of each fossil by their temporal distances to the most recent fossil. They then calculate confidence intervals by generating 10{\small,}000 estimates and taking the sample quantile for the interval's endpoints \cite{Bradshaw2012}.

Suppose our observed fossil record $\bm{x} = [x_1, x_2, \dots, x_n]^\top$, with each fossil having a corresponding standard deviation denoted by $\bm{\sigma}=[\sigma_1, \sigma_2, \dots, \sigma_n]^\top$, which represents the variation introduced by radiometric error.

First, resample each fossil according to $X^*_i \sim \mathcal{N}(x_i, \sigma_i^2)$ and sort the resulting set of resamples; denote the resampled fossil record by $\bm{x^*} = [x^*_1, x^*_2, \dots, x^*_n]^\top$.

Next, suppose that the most-recent fossils are more influential on the sighting rate as extinction approaches. We can therefore apply the \citet{Mcinerny2006} method as per \autoref{eqn:mcinerny-estimator} (or \autoref{eqn:mcinerny-bias-corrected-estimator} if applying the correction proposed by \citet{Huang2019}) to obtain $\hat\theta_{k; \text{MI}}$ using the $k$ most recent fossils in the record. This is repeated for every $k \in \{2, 3, \dots, n\}$, resulting in $n-1$ McInerny estimates. For each estimate, compute a weight $\omega_k$ such that each estimate is inversely weighted according to the temporal distance between the interval used and the most recent gap $x^*_{2} - x^*_{1}$: \[
\omega_k = \frac{x^*_{2} - x^*_{1}}{x^*_{k} - x^*_{1}}
\]
Thus, the GRIWM estimator $\hat\theta_{\text{GRIWM}}$ is calculated as a weighted average of the $n-1$ McInerny estimates:\begin{equation}\label{eq:griwm1}
    \hat\theta_{\text{GRIWM}} = \frac{\sum_{k=2}^{n} \omega_k \hat\theta_{k; \text{MI}}}{\sum_{k=2}^{n} \omega_k}
\end{equation}

To calculate confidence intervals, 10{\small,}000 estimates of $\hat\theta_{\text{GRIWM}}$ are generated, and the appropriate sample quantiles are taken. For example, the point estimate of the extinction time would be found by taking the median of the 10{\small,}000 estimates, and a 95\% confidence interval would be constructed by taking sample quantiles at $q=0.025$ and $q=0.975$.

The GRIWM estimation method has some notable caveats as a result of the derivation from the McInerny method as well as only accounting for the variation introduced by measurement error and ignoring sampling error. We note that the GRIWM estimate of $\theta$ is positively biased, as the bias is carried over from the \citet{Mcinerny2006} method. This can be corrected for by applying the adjustment proposed by \citet{Huang2019} as per \autoref{eqn:mcinerny-bias-corrected-estimator}, as an implementation using \autoref{eqn:mcinerny-estimator} will result in a positively biased estimator. Another problem carried over from the McInerny method is the choice of the threshold probability $q$, which the authors suggested to be 0.05. As was discussed in the previous section, we suggest changing this to 0.5 to produce median-based estimates.

\section{Simulated-Inversion Estimator}

We now describe the simulated-inversion (SI) estimator, an unpublished method proposed by \citet{Huang2019} in his 2019 thesis. This method is based on the technique of test-statistic inversion for generating confidence intervals for an unknown parameter $\theta \in \R$ \cite{Carpenter1999}. Before we proceed with the SI estimator, we first introduce test statistic inversion.

Let $\bm{W}$ denote a random vector of $n$ independent and identically distributed random variables from a distribution with CDF $F_X (\cdot; \theta)$. Let $S(\bm{W})$ be a test statistic that is stochastically increasing in $\theta$. Denote $\bm{W}$ and $S(\bm{w})$ as a realisation of $\bm{W}$ and $S(\bm{w})$ respectively, and let $\alpha$ be a fixed significance level for the test:
\begin{equation}
    \theta_1 < \theta_2 \implies F_S(\theta_1) < F_S(\theta_2) 
\end{equation}

where $F_S(\theta) = \PP_\theta\left\{S(\bm{W}) > S(\bm{w})\right\}$. Then, a central $100(1-\alpha)\%$ confidence interval denoted by $[L, U]$ can be found, where $L$ and $U$ satisfy: \begin{equation}
\begin{aligned}
    \PP_{\theta = L}\left\{S(\bm{W}) > S(\bm{w})\right\} &= \alpha/2 \\
    \PP_{\theta = U}\left\{S(\bm{W}) > S(\bm{w})\right\} &= 1 - \alpha/2
\end{aligned}
\end{equation}

More generally, this method can be used to find some quantile $q$: \begin{equation}\label{eq: inversion}
    \PP_{\theta = \theta_q}\left\{S(\bm{W}) > S(\bm{w})\right\} = q
\end{equation}

This method of constructing confidence intervals exploits the duality between confidence intervals and hypothesis tests. Consider a two-sided test of the null hypothesis $H_0: \theta = \theta_0$ using test statistic $S(\bm{w})$. A $100(1-\alpha)\%$ confidence interval is the set of hypothesised values for $\theta_0$ where the test is not significant at level $\alpha$. The duality refers to the way $\theta_0$ is always in the $100(1-\alpha)\%$ confidence interval if the test is not significant at $\alpha$.

Thus, by inverting the test's acceptance region so that the region is a function of $\theta$ rather than being a function of the test-statistic, a confidence interval may be constructed. To guarantee a unique solution, inversion assumes that the quantile function $\PP_{\theta = \theta}\left\{S(\bm{W}) > S(\bm{w})\right\}$ is stochastically increasing in $\theta$ (\autoref{fig:inversion_diagram}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/inversion-diagram.png}
    \caption{The confidence interval - hypothesis test duality. The $100(1-\alpha)\%$ confidence interval for $\theta$, denoted by $[L, U]$, is equivalent to the set of possible values for $\theta$ such that the test statistic $S(\bm{W})$ is in the acceptance region for the null hypothesis. This is only true subject to a stochastically increasing in $\theta$ assumption.}
    \label{fig:inversion_diagram}
\end{figure}

Returning to the simulated-inversion (SI) estimator proposed by \citet{Huang2019}, consider a data generation process following model $M$, which may or may not account for measurement error. The goal is to estimate $\PP_\theta\left\{S(\bm{W}) > S(\bm{w})\right\}$ by simulation, and then construct confidence intervals by inversion. These two steps are described below:

\begin{enumerate}
    \item \textbf{Simulation}: Suppose we have a given dataset $\bm{w} = [w_1, w_2, \dots, w_n]^\top$ and a large, known set of $r$ potential values for the true extinction time $\bm{\theta} = [\theta_1, \theta_2, \dots, \theta_r]^\top$. Then, for each potential value $\theta_i$, simulate a pseudo dataset $\bm{w}^*_i$ from the model $M$ specified by $\theta_i$ and take a sample statistic $S^*_i$ from the pseudo dataset. Thus, for a given dataset of $n$ observations and a set of $r$ potential values for $\theta$, a set of $r$ simulated statistics $\bm{S}^*$ can be obtained.
    \item \textbf{Inversion}: An estimate for $\PP_\theta\left\{S(\bm{W}) > S(\bm{w})\right\}$ is constructed by regressing the indicator $\mathbbm{1}\left\{ S(\bm{W}^*_i) > S(\bm{w}^*_i) \right\}$ against $\theta_i$ for all $i \in \left\{ 1, \dots, r \right\}$. Finally, $\theta$ can be estimated by inverting the estimated curve.
\end{enumerate}

This method of extinction estimation is very general, as it only requires that the statistic $S(\bm{W})$ is stochastically increasing in $\theta$. It imposes no other restrictions on the statistic $S(\bm{W})$ used to construct the interval, and imposes no restrictions on the simulation model used to estimate $\PP_\theta\left\{S(\bm{W}) > S(\bm{w})\right\}$, meaning it can be applied regardless of the complexity of the data generation model. Thus, the SI estimator could be used for broader settings beyond the uniform distribution assumption, and could readily account for the presence of measurement errors. For example, \citet{Huang2019} suggested the Simulated Inversion - Uniform Gaussian Minimum (SI-UGM) estimator which uses the simulated inversion estimator with the minimum statistic and simulates fossils as uniformly distributed with Gaussian measurement errors. \citet{King2020} extended this approach by adapting the simulation model to account for calibration error, demonstrating the flexibility of the SI estimator.

That being said, there are some considerations when using the SI estimator. Since the true extinction time is unknown and will be different in each application, the interval of potential values for $\theta$ must be wide and there is currently no obvious robust approach to choosing this interval. In \autoref{chap: proposed-methods}, we treat this as a stochastic optimization problem, using the Robbins-Monro process to circumvent this issue. Moreover, in the inversion step, \citet{Huang2019} estimated $\PP_\theta\left\{S(\bm{W}) > S(\bm{w})\right\}$ using a regression method that is not necessarily monotonic. By enforcing monotonicity, the method becomes more robust to different simulation models as shown by \citet{King2020}, who demonstrated that monotonicity enforcement improved accuracy and efficiency.