
In this chapter, we discuss some commonly used assumptions and methods in the literature for estimating extinction times, while formulating the notation that will be used in later chapters. This background knowledge will form a baseline understanding of the challenges faced by paleontologists and the approaches developed so far to tackle these challenges.

\section{Common Assumptions}

Owing to the complexity of fossilisation, fossil recovery, and radiocarbon dating, a number of simplifying assumptions are often made. We review two common assumptions: uniform fossil preservation/recovery and normally distributed measurement errors.

\subsection{Uniform Fossil Deposition and Recovery}\label{ssec: ass_unif}

Let the true fossil dates be denoted by random variables $\bm{X} = [X_1, \dots, X_n]^\top$, where $X_i$ is the number of years before present (BP). Then, by also assuming that the fossil recovery rate $\lambda$ is constant, we have fossil dates that are generated by a homogeneous Poisson process. A property of this process results in uniformly distributed fossil dates (conditional on $n$, which is a Poisson random variable) and exponentially distributed time gaps between fossil dates:
\begin{align*}
    X_i  &\overset{i.i.d}{\sim} \mathcal{U}(\theta, K) \\
    G_i &\overset{i.i.d}{\sim} \exp{(\lambda)}
\end{align*}
for $i = 1, 2, \dots, n$, where  $n \sim \textrm{Pois}(\lambda(K-\theta))$. Let $\theta$ be an unknown parameter of interest representing the extinction time of the species, and let $K$ be the start of the time period of interest. For chapters 2-5 we will assume $K$ is a \textit{known} value representing the earliest possible date a fossil can be obtained. This may be interpreted as the speciation date; however, in practice, $K$ can be any abritrarily selected upper bound as long as it is between the speciation date and extinction date.

Uniform fossil recovery is a strong assumption and is generally not true; however, in spite of the consensus around the invalidity of this assumption, the majority of analyses to date continue to make this assumption. These ``first-generation" methods (\textcite{WangMarshall2016} coined the term ``first-generation" for methods using a uniform assumption) persist due to the lack of better alternatives: although methods that infer recovery rates from data exist, it is more often the case that the required quantitative knowledge of recovery rates are simply unavailable, rendering them inapplicable.  Furthermore, the clarity, simplicity, and convenience of uniform assumptions allow ``first-generation" methods to persist in the literature.

A proposed alternative to the uniform recovery assumption is the use of a ``reflected beta distribution" described in \textcite{Wang2016}. Since recovery rates tend to be higher in the middle of a species' life span and taper down near both the beginning and the end of a species' life \parencite{Lee2010}, a beta distribution could be used to describe scenarios where fossil recovery rates are increasing or decreasing prior to extinction.

\subsubsection{Aside: First, Second, and Third Generation Methods}

The aforementioned groupings formulated by \textcite{WangMarshall2016} separate extinction time estimation methods into first, second, and third generation methods. First generation methods assume uniform fossil deposition and recovery, which greatly simplify computation and estimation. Second generation methods allow for a non-uniform assumption by inferring deposition and recovery rates from the fossil data - the GRIWM method developed by \textcite{Bradshaw2012} is one such method. Although GRIWM also implicitly relies on a uniform fossil distribution, meaning it should be a ``first generation" model, it can be argued that its ability to account for measurement error means GRIWM infers the recovery rate from the observed fossil record and each fossil's standard errors, hence justifying its second-generation categorisation. Finally, third generation methods attempt to model fossil deposition using stratigraphic and environmental data --- unsurprisingly, these methods are rarely applied as the volume of data required often make them impractical.

\subsection{Normally Distributed Measurement Error}

Suppose that our true fossil dates $\bm{X} = [X_1, \dots, X_n]^\top$ each possess corresponding measurement errors $\bm{\varepsilon} = [\varepsilon_1, \dots, \varepsilon_n]^\top$, which are assumed to be independent and normally distributed with mean 0:

\begin{align*}
    \text{\textit{True} fossils:} \quad X_i  &\overset{i.i.d}{\sim} \mathcal{U}(\theta, K) \\
    \text{Measurement errors:} \quad \varepsilon_i &\overset{i.i.d}{\sim} \mathcal{N}(0, \sigma^2) \\
    \text{\textit{Observed} fossils:} \quad W_i &= X_i + \varepsilon_i 
\end{align*}

where $X_i$ and $\varepsilon_i$ are independently distributed for all $i = 1, 2, \dots, n$.

Although it is common to assume that measurement error follows a normal distribution, there is evidence to suggest this is not necessarily true. The measurement errors $\bm{\varepsilon}$ can be thought of having two sources: from the radiocarbon dating process, which measures the amount of radiocarbon in organic matter to give the age of the fossil in \textit{radiocarbon} years. There is some evidence to suggest that these errors are normally distributed \parencite{Walker2005Quaternary, Taylor1987}. However, these radiocarbon years must then be converted to calendar years according to calibration curves generated by statistical methods (the newest standard being IntCal20, which was published in 2020 \parencite{Reimer2020}). The errors introduced by calibration curves are not necessarily normal though, and there is ample evidence in the literature showing this \parencite{Ramsey2009, Ramsey2010, Ramsey2013}.

Existing methods often assume that measurement error introduced by the fossil dating process is negligible relative to other sources of variability, such as sampling error. This assumption is not generally applicable, as some fossil records will have dating errors that are comparable to the gaps between fossils \parencite{Solow2006}, and this assumption should be checked prior to application of estimation methods. %In \autoref{chap: experiments}, we will show the effect of radiometric error on various methods' point estimates and confidence intervals.

\section{Current Methods}

We outline three methods for estimating extinction times and getting confidence intervals for said estimates. These methods all assume uniform fossil deposition and recovery, with GRIWM/BRIWM being the only one that deals with measurement error.

\subsection{Maximum Likelihood Estimator (MLE)}

We begin by describing maximum likelihood estimation as a method for inferring the extinction time of a species. Under a uniform recovery assumption and no measurement error, the Maximum Likelihood Estimator (MLE) is the first order statistic $X_{(1)}$:
\begin{equation}\label{eq:mle}
    \hat\theta_{\text{MLE}} = X_{(1)}
\end{equation}

From a statistical perspective, the MLE procedure is an obvious choice to estimate the extinction time, as one can intuit the most recent fossil as representing the most information about a species' extinction date. However, the incompleteness of the fossil record means that the first and last fossil will never precisely represent the introduction and extinction times of a species, a paleontological principle called the ``Signor-Lipps Effect" \parencite{Signor1982}. Perhaps for this reason (along with its unrealistic assumptions) maximum likelihood sees limited use in the literature \parencite{Ludwig1996, Jaric2016}. Nonetheless, the MLE serves as a straightforward baseline for estimating extinction times.

Consider the likelihood function of the uniform distribution
\[
\mathcal{L}(\theta | \bm{x}) =  \frac{n}{(K - \theta)^{n+1}} > 0 \quad \forall \theta
\]

where $K$ (the upper bound for $X_i$) is a known constant. Differentiating the likelihood, we can show that the derivative is a monotonically increasing function of $\theta$:
\[
\frac{d}{d\theta}\mathcal{L}(\theta | \bm{x} ) = \begin{cases}
    \frac{1}{(K-\theta)^n} & \text{if $X_{(1)} \geq \theta$ and $X_{(n)} \leq K$} \\
    0 & \text{otherwise}
\end{cases}
\]

where $X_{(i)}$ is the $i$\textsuperscript{th} order statistic.

Thus, our maximum likelihood estimator $\hat\theta_{\text{MLE}}$ is the sample minimum $x_{(1)}$. However, this estimator is positively biased as the most recent fossil is always at least as old as the species' extinction date:
\[
    \E[\hat\theta_{\text{MLE}}] = \E[X_{(1)}] = \frac{K}{n+1} + \frac{n}{n+1}\theta
\]

Correcting for this gives us an unbiased MLE:
\begin{equation}\label{eq:ubmle}
    \hat\theta_{\text{UBMLE}} = X_{(1)} \frac{n+1}{n} - \frac{K}{n}
\end{equation}

\subsection{Strauss Estimator}

\textcite{Strauss1989} proposed an unbiased estimator that did not require a known upper bound $K$, instead treating both end points of the distribution ($\theta$ and $K$) as unknown parameters. The Strauss estimator for the extinction time is based on the maximum likelihood procedure --- the MLE for $\theta$ is the first order statistic as before, and by symmetry the MLE for $K$ is the $n$\textsuperscript{th} order statistic:
\begin{align*}
    \hat\theta_{\text{MLE}} &= X_{(1)} \\
    \hat K_{\text{MLE}} &= X_{(n)}
\end{align*}

Next, to find an unbiased estimate, we take the expectation of both estimators (see appendix for proof):
\begin{align*}
    \E \left[ \hat\theta_{\text{MLE}} \right] &= \frac{K + n\theta}{n+1} \\
    \E \left[ \hat K_{\text{MLE}} \right]  &= \frac{\theta + nb}{n+1}
\end{align*}

Solving the equations simultaneously, $K$ can be eliminated to yields the following:
\[
\E \left( \frac{nX_{(1)} - X_{(n)}}{n-1} \right) = \theta
\]

which is then used to derive the following unbiased Strauss estimator as an alternative to the previously found unbiased MLE:
\begin{equation}\label{eq:strauss}
\hat\theta_{\text{Strauss}} = \frac{n X_{(1)} - X_{(n)}}{n-1}
\end{equation}

% In the same paper, Strauss \& Sadler were able to construct confidence intervals for either end point of the following form:

% \[
% \textcolor{red}{TBD}
% \]

% However, it has been shown that its high sensitivity to low number of records causes overly wide confidence intervals and makes the model inefficient.

\subsection{McInerny Estimator}

\textcite{Mcinerny2006} developed a method for inferring extinction based on previous sightings, assuming that sightings are generated by a Poisson process and thus the dates are uniformly distributed, conditional on $n$. The authors' proposed method assumes a constant recovery rate $\lambda$ and models the probability of another fossil\footnote{Note that these papers were concerned with estimating the extinction of species that have been seen relatively recently, but their methods can be similarly applied to a paleontology context where a species is almost certainly already extinct. For the remainder of this section, we will refer to fossils, rather than sightings.} being found, $p$, as a function of $\lambda$ and the time gap since the last observation:
\[
p = \left( 1 - \lambda \right)^{x_1 - \theta} \quad;\quad \lambda = \frac{n-1}{K - x_1}
\]

where the recovery rate $\lambda$ is estimated as the number of samples between $K$ and $x_1$ divided by the time interval spanned by the observations.

To find the extinction time, consider the extinction time $\theta$ as the terminal record --- that is, the date at which the probability of another fossil being found is less than some threshold probability $q$. Then, the above probability $p$ can be used to estimate the extinction time iteratively. First, let $\theta = x_1 - 1$ and calculate $p$. Then, if $p > q$, decrement $\theta$ by 1 and recalculate $p$. If $p < q$, then the terminal fossil has been reached and we have obtained $\hat\theta_{\text{McInerny}}$; otherwise, continue to iterate. Thus, the McInerny et al. estimator is:

\begin{equation}
    \hat\theta_{\text{MI}; n, q} = \min\left\{ \hat\theta ; p = \left( 1 - \frac{n-1}{K - x_1} \right)^{x_1 - \hat\theta} < q \right\}
\end{equation}

There are a number of issues with this method for estimating extinctions in the fossil record. First, a known property stationary Poisson processes is that the gaps between observations should be exponentially distributed; however, the results from their paper show a geometric distribution with rate $\lambda$ instead. This is because McInerny et al. implicitly assume that time is discretised, an assumption that is not necessarily valid as sightings are not often counted in discrete time units.

Secondly, there are significant questions around the validity of modeling $\lambda$ using the interval $[x_1, K]$, as the period of interest should be bounded by $\theta$ instead of $x_1$. Thus, $\lambda$ should instead be modelled as \[ \lambda = \frac{n}{K - \theta} \] This formulation may have developed out of the author's conservation biology background, as they sought to answer the questions: ``Given a time series of sightings within a time period, is this species extinct? If so, when might extinction have occurred?" and hence designed it to work with a known ``observation period" in which some $n$ sightings were observed. This differs to a paleontology context, where the ``observation period" is the interval in which the species existed, $[\theta, K]$. To compute the probability of another fossil being recovered would therefore require knowledge of this lower bound $\theta$, which is unfortunately unknown. Thus, to circumvent this problem, McInerny et al. suggest artificially bounding the data, using the most recent observation $x_1$ as a lower bound for the observation period and reducing the number of sightings, $n$, to $n-1$. 

Finally, McInerny et al.'s method of estimating $\theta$ is dependent on the number of fossil dates --- they note that the number of samples required to achieve a 95\% certainty of extinction may increase beyond what is feasible, making the method less applicable for estimating extinction from fossils. Furthermore, McInerny et al. give every fossil in the time series equal contribution to the estimated extinction time (since sighting rate is calculated using the whole time series), which may not necessarily be true.

\subsection{GRIWM Estimator}

The Gaussian-Resampled Inverse-Weighted McInerny (GRIWM) estimator is an approach based on the previous \textcite{Mcinerny2006} method, designed to account for both the influence of sighting rate and dating uncertainty. \textcite{Bradshaw2012} assume uniformly distributed fossils and Gaussian-distributed measurement errors for their procedure, which has two main ideas: one, use Gaussian resampling to account for the known radiometric uncertainty associated with each fossil; and two, use the McInerny method to estimate the true extinction time, inversely weighting the contribution of each fossil by their temporal distances to the most recent fossil. They then calculate confidence intervals by generating 10,000 estimates and taking the sample quantile for the interval's endpoints \parencite{Bradshaw2012}.

Suppose we have an observed fossil record $\bm{x} = [x_1, x_2, \dots, x_n]^\top$, with each fossil having a corresponding measurement error uncertainty $\bm{\sigma}=[\sigma_1, \sigma_2, \dots, \sigma_n]^\top$.

First, resample each fossil according to $X^*_i \sim \mathcal{N}(x_i, \sigma_i^2)$ and sort the resulting set of resamples; denote the resampled fossil record by $\bm{x^*} = [x^*_1, x^*_2, \dots, x^*_n]^\top$.

Next, consider that the most-recent fossils are more influential on the sighting rate as extinction approaches. Thus, we can apply the McInerny et al. (2006) method to the $k$ most recent fossils, for all $k \in \{2, 3, \dots, n-1\}$ with threshold probability $q$. This results in $n-2$ McInerny estimates: $\hat\theta_{\text{MI}; 2, q}, \hat\theta_{\text{MI}; 3, q}, \dots, \hat\theta_{\text{MI}; n-1, q}$. The final estimator can be found by computing a weighted average where the weight $w_k$ is the ratio of the interval between the two most recent fossils and the chosen interval:

\[
w_k = \frac{x^*_{2} - x^*_{1}}{x^*_{k} - x^*_{1}}
\]

Thus, the weighted estimator $\hat\theta_{q}$ is calculated as a weighted average over all possible records:

\begin{equation}\label{eq:griwm1}
    \hat\theta_{\text{GRIWM}; q} = \frac{\sum_{k=2}^{n-1} w_k \hat\theta_{\text{MI}; k, q}}{\sum_{k=2}^{n-1} w_k}
\end{equation}

To calculate confidence intervals, 10,000 estimates of $\hat\theta_{\text{GRIWM}; q}$ are generated, and the appropriate quantiles are taken. A point estimate of the extinction time would be at the 0.5\textsuperscript{th} quantile.

% \subsubsection{Bias-Corrected GRIWM}

% Huang (2019) proposed a correction to this method as the estimator proposed by McInerny et al. is positively biased \parencite{Huang2019}. This correction replaces the denominator used in the estimated recovery rate $\hat\lambda_k$ with $x_{k} - \theta$ instead of $x_{k} - x_{1}$, resulting in a bias-corrected expression for the estimated gap:

% \begin{equation}\label{eq:ub-mcinerny}
% g_{k, q; \text{corrected}} = \frac{k + \log(q)}{k} g_{k, q}
% \end{equation}

% We can substitute this corrected value into \autoref{eq:griwm1} to get a new estimate for $\hat\theta_{\text{GRIWM}}$

% \subsubsection{BRIWM Estimator}

% Saltr\'e et al., 2015 \parencite{Saltre2015} developed a variant of GRIWM called BRIWM, which uses \textbf{B}ootstrap resampling instead of \textbf{G}aussian resampling. Rather than resampling each fossil into the standard deviation, BRIWM uses a bootstrap technique with replacement, neglecting dating errors. This method assumes that $\theta$ can be inferred from a subsample of the original fossil time series, which may not always be true and was specifically devised to counter the assumption that all data points are equally reliable in terms of data quality. That being said, Saltr\'e et al.'s results suggest that GRIWM's ability to explicitly account for radiocarbon dating prevails over BRIWM accounting for record reliability.