
In this chapter, we will describe some commonly used assumptions and methods in the literature for estimating extinction times. We will discuss the validity of these assumptions and methods, and any observed pitfalls or criticisms. This background knowledge will form a baseline understanding of the challenges faced by paleontologists and the approaches developed so far to tackle these challenges, and inform the methods proposed in \autoref{chap: inversion}.

\section{Common Assumptions}

Owing to the complexity of fossilisation, fossil recovery, and radiocarbon dating, a number of simplifying assumptions are often made. We will review two common assumptions made in the literature, and discuss their validity in terms of estimating extinction times. We will also formulate some of the notation that will be used in this thesis.

\subsection{Uniform Fossil Deposition and Recovery}\label{ssec: ass_unif}

Many existing methods used in paleontology assume that the \textit{true} ages of fossils are independent and uniformly distributed over the interval [$\theta$, $K$], where the lower bound $\theta$ is the extinction time and the upper bound $K$ is the speciation or invasion time represented by years before present. This assumption is a strong one, and is generally not valid over the entire fossil record as there are in fact two events being assumed: that fossils are deposited at a constant rate and that the species in question has constant abundance over the interval of interest \parencite{Lee2010}. 

This assumption supposes that fossils are generated by a homogeneous Poisson process. Thus, the number of samples in a fixed interval [$\theta$, $K$] is a Poisson random variable. Each sample (call the $i\textsuperscript{th}$ sample $U_i$) has an equal chance of being recovered in this interval, and are therefore independent and uniformly distributed, conditional on $n$. The gaps between intervals, denoted by $G_i$ for the $i\textsuperscript{th}$ gap, are independent and exponentially distributed at rate $\lambda$. This assumption can be expressed as follows \parencite{Strauss1989}:

\begin{align*}
    U_i  &\overset{i.i.d}{\sim} \mathcal{U}(\theta, K) \\
    G_i &\overset{i.i.d}{\sim} \exp{(\lambda)}
\end{align*}

for $i = 1, 2, \dots, n$, where  $n \sim \textrm{Pois}(\lambda(K-\theta))$. The endpoints $\theta$ and $K$ represent the extinction and speciation/invasion times, respectively, and either may be considered as unknown.

\subsubsection{Aside: First, Second, and Third Generation Models}

Wang et al formulated groupings for the various extinction time estimation methods, separating them into "first", "second", and "third" generation methods \parencite{Wang2016}. "First-generation" methods assume uniform fossil deposition and recovery, which greatly simplify computation and estimation. Second generation methods allow for a non-uniform assumption by inferring deposition and recovery rates from the fossil data - the GRIWM method developed by Bradshaw et al \parencite{Bradshaw2012} is one such methods. Although GRIWM also implicitly relies on a uniform fossil distribution technically means it should be a "first generation" model, its ability to account for measurement error makes it somewhat more advanced than most of the "first generation" methods. Finally, third generation methods attempt to model fossil deposition using stratigraphic and environmental data -- perhaps unsurprisingly, these methods are rarely applied as  the volume of data required often make them impractical.

\subsection{Perfect Radiocarbon Dating}

Existing methods often assume that measurement error introduced by the fossil dating process is negligible relative to other sources of variability, such as sampling error. This assumption is not generally applicable, as some fossil records will have dating errors that are comparable to the gaps between fossils \parencite{Solow2006}, and this assumption should be checked prior to application of estimation methods.

One of the most common methods used for fossil dating is radiocarbon dating. This method measures the amount of radiocarbon in organic matter, which decays from death at a known rate, to give the age of the fossil in radiocarbon years. These radiocarbon years are then converted to calendar years according to calibration curves generated by statistical methods (the newest standard being IntCal20, which was published in 2020) \parencite{Reimer2020}. This process of radiocarbon dating and calibration introduces measurement errors that are approximately consistent with a zero-mean normal distribution \parencite{Walker2005Quaternary}. We can express the effect of normally distributed measurement errors on our observed fossils as follows:

\begin{align*}
    \text{\textit{True} fossils:} \quad U_i  &\overset{i.i.d}{\sim} \mathcal{U}(\theta, K) \\
    \text{Measurement errors:} \quad \varepsilon_i &\overset{i.i.d}{\sim} \mathcal{N}(0, \sigma^2) \\
    \text{\textit{Observed} fossils:} \quad X_i &= U_i + \varepsilon_i 
\end{align*}

where $U_i$ and $\varepsilon_i$ are independently distributed for all $i = 1, 2, \dots, n$ and the radiocarbon errors $\varepsilon_i$ are normally distributed with constant variance $\sigma^2$.

In \autoref{chap: experiments}, we will show the effect of radiometric error on various methods' point estimates and confidence intervals.

\section{Current Methods}

We outline three methods for estimating extinction times and getting confidence intervals for said estimates. These methods all assume uniform fossil deposition and recovery, with GRIWM/BRIWM being the only one that deals with measurement error.

\subsection{(Bias-adjusted) Maximum Likelihood Estimator (MLE)}

The Maximum Likelihood Estimator (MLE) is a "first-generation" method which ignores measurement error. When assuming no measurement error and uniform fossil deposition (i.e. $X_i = U_i$), the MLE of $\theta$ is the first order statistic $X_{(1)}$:

\begin{equation}\label{eq:mle}
    \hat\theta_{\text{MLE}} = X_{(1)}
\end{equation}

The derivation of the MLE is as follows. A uniform distribution has the below likelihood function (where $X_{(k)}$ is the $k^{th}$ order statistic):

\[
\frac{d}{d\theta}\mathcal{L} = \begin{cases}
    \frac{1}{(K-\theta)^n} & \text{if $X_{(1)} \geq \theta$ and $X_{(n)} \leq K$} \\
    0 & \text{otherwise}
\end{cases}
\]

where $K$ (the upper bound for $X_i$) is a known constant. Differentiating the likelihood, we can show that the derivative is a monotonically increasing function of $\theta$:

\[
\mathcal{L}(\theta | \bm{x}) =  \frac{n}{(K - \theta)^{n+1}} > 0 \quad \forall \theta
\]

And therefore our maximum likelihood estimator $\hat\theta_{\text{MLE}}$ is the sample minimum $x_{(1)}$. This result can be intuited as the most recent fossil holding the most information about a species' extinction date --- however, this estimator is positively biased as the most recent fossil is always at least as old as the species' extinction date:

\[
    \E[\hat\theta_{\text{MLE}}] = \E[X_{(1)}] = \frac{K}{n+1} + \frac{n}{n+1}\theta
\]

Correcting for this gives us an unbiased MLE:

\begin{equation}\label{eq:ubmle}
    \hat\theta_{\text{UBMLE}} = X_{(1)} \frac{n+1}{n} - \frac{K}{n}
\end{equation}

\subsection{Strauss Estimator}

Strauss \& Sadler (1989) \parencite{Strauss1989} proposed an unbiased estimator that did not require knowing the speciation date, instead treating both end points of the distribution ($\theta$ and $K$) as unknown parameters. The MLE for $\theta$ is as shown above, and by symmetry the MLE for $K$ is the largest order statistic:

\begin{align*}
    \hat\theta_{\text{MLE}} &= X_{(1)} \\
    \hat K_{\text{MLE}} &= X_{(n)}
\end{align*}

Next, to find an unbiased estimates, we take the expectation of both estimators (see appendix for proof):

\begin{align*}
    \E \left[ \hat\theta_{\text{MLE}} \right] &= \frac{b + n\theta}{n+1} \\
    \E \left[ \hat K_{\text{MLE}} \right]  &= \frac{\theta + nb}{n+1}
\end{align*}

Solving the equations simultaneously, $K$ can be eliminated to yields the following:

\[
\E \left( \frac{nX_{(1)} - X_{(n)}}{n-1} \right) = \theta
\]

which is then used to derive the following unbiased Strauss estimator as an alternative to the previously found bias-correct MLE:

\begin{equation}\label{eq:strauss}
\hat\theta_{\text{Strauss}} = \frac{n X_{(1)} - X_{(n)}}{n-1}
\end{equation}

% In the same paper, Strauss \& Sadler were able to construct confidence intervals for either end point of the following form:

% \[
% \textcolor{red}{TBD}
% \]

% However, it has been shown that its high sensitivity to low number of records causes overly wide confidence intervals and makes the model inefficient.

\subsection{McInerny Estimator}

McInerny et al. (2006) developed a method for inferring extinction based on previous sightings \parencite{Mcinerny2006}, assuming that sightings are uniformly distributed, conditional on $n$. More specifically, it assumes the data is generated by a stationary Poisson process, an assumption derived from a previously published method by Solow (1993) \parencite{Solow1993}. Unlike the MLE and Strauss estimators, we denote the sighting record by an ordered set of sighting times $\mathbf{t} = (t_1, t_2, \dots, t_n)$ on the open interval $(0, T)$ where $t_n$ is the most recent sighting and $T$ is some constant.

The Solow equation expresses the probability of fossil\footnote{Note that these papers were concerned with estimating the extinction of species that have been seen relatively recently, but their methods can be similarly applied to a paleontology context where a species is almost certainly already extinct. For the remainder of this section, we will refer to fossils, rather than sightings.} recovery as equally likely in the period $T$:

\[
p = \left( t_n/T \right)^n
\]

However, McInerny et al. hypothesised that, for more recently discovered species, the time since $t_n$ may be more informative than the value found by the above Solow equation \parencite{Mcinerny2006}. Thus, they proposed a new equation for the probability of recovering another fossil given the previous recovery rate \textit{and} the time since the last observation ($T - t_n$):

\[
p = \left( 1 - \left( (n-1)/t_n \right) \right)^{T - t_n}
\]

where $\frac{n}{t_n}$ is the recovery rate, estimated as the number of samples divided by the time interval. However, McInerny et al. note that it is rarely possible to select a start date for the period $T$. Thus, in the absence of this knowledge, they use the $t_n$ as the starting point, and so the number of samples is $n-1$ instead of $n$.

To find the extinction time, consider that the extinction time $\theta$ is also the terminal record --- that is, the record where the probability of another fossil is less than some threshold probability $q$. The above probability equation $p$ can be used to estimate the extinction time iteratively: if $p > q$, increment $T$ by 1 and recalculate. If $p < q$, then the terminal fossil has been reached and we have obtained $\hat\theta_{\text{McInerny}}$; otherwise, continue to iterate \parencite{Bradshaw2012}. Thus, the McInerny et al. estimator is:

\begin{equation}
    \hat\theta_{\text{MI}; n, q} = \min\left\{ T ; p = \left( 1 - \left( (n-1)/t_n \right) \right)^{T - t_n} < q \right\}
\end{equation}

Clearly, McInerny et al.'s method of estimating $\theta$ is dependent on the number of samples used --- Bradshaw et al. (2012) hypothesised that the most recent records in a fossil time series would be more influential on the sighting rate as extinction is approached, proposing a method that inversely weights the contribution of each dated record to $\hat\theta$ depending on its time gap from the most recent record \parencite{Bradshaw2012}. This suggestion was implemented in their method, GRIWM.

\subsection{GRIWM Estimator}

The Gaussian-Resampled Inverse-Weighted McInerny (GRIWM) estimator is a recently developed approach based on the previous method proposed by McInerny et al. (2006). Bradshaw et al. assume uniformly distributed fossils and Gaussian-distributed measurement errors for their procedure, which has two main ideas: one, given observed radiometric uncertainty for each fossil sample, use Gaussian resampling to account for said uncertainty; and two, use the McInerny method to estimate the true extinction time, inversely weighting the contribution of each fossil by their temporal distances to the most recent fossil. They then calculate confidence intervals by generating 10,000 estimates and taking the sample quantile for the interval's endpoints \parencite{Bradshaw2012}.

Suppose we have an observed fossil record $\bm{x} = [x_1, x_2, \dots, x_n]^\top$, with each fossil having a corresponding measurement error uncertainty $\bm{\sigma}=[\sigma_1, \sigma_2, \dots, \sigma_n]^\top$.

First, resample each fossil according to $X^*_i \sim \mathcal{N}(x_i, \sigma_i^2)$ and sort the resulting set of resamples; denote the resampled fossil record by $\bm{x^*} = [x^*_1, x^*_2, \dots, x^*_n]^\top$.

Next, consider that the most-recent fossils are more influential on the sighting rate as extinction approaches. Thus, we can apply the McInerny et al. (2006) method to the $k$ most recent fossils, for all $k \in \{2, 3, \dots, n\}$ with threshold probability $q$. This results in $n-1$ estimates of $\theta$: $\hat\theta_{\text{MI}; 1, q}, \hat\theta_{\text{MI}; 2, q}, \dots, \hat\theta_{\text{MI}; n, q}$. The final estimator can be found by computing a weighted average where the weight $w_k$ is the ratio of the interval between the two most recent fossils and the chosen interval:

\[
w_k = \frac{x^*_{2} - x^*_{1}}{x^*_{k} - x^*_{1}}
\]

Thus, the weighted estimator $\hat\theta_{q}$ is calculated as a weighted average over all possible records:

\begin{equation}\label{eq:griwm1}
    \hat\theta_{\text{GRIWM}; q} = \frac{\sum_{k=2}^n w_k \hat\theta_{\text{MI}; k, q}}{\sum_{k=2}^n w_k}
\end{equation}

To calculate confidence intervals, 10,000 estimates of $\hat\theta_{\text{GRIWM}; q}$ are generated, and the appropriate quantiles are taken. A point estimate of the extinction time would be at the 0.5\textsuperscript{th} quantile.

\subsubsection{Bias-Corrected GRIWM}

Huang (2019) proposed a correction to this method as the estimator proposed by McInerny et al. is positively biased \parencite{Huang2019}. This correction replaces the denominator used in the estimated recovery rate $\hat\lambda_k$ with $x_{k} - \theta$ instead of $x_{k} - x_{1}$, resulting in a bias-corrected expression for the estimated gap:

\begin{equation}\label{eq:ub-mcinerny}
g_{k, q; \text{corrected}} = \frac{k + \log(q)}{k} g_{k, q}
\end{equation}

We can substitute this corrected value into \autoref{eq:griwm1} to get a new estimate for $\hat\theta_{\text{GRIWM}}$

\subsubsection{BRIWM Estimator}

Saltr\'e et al., 2015 \parencite{Saltre2015} developed a variant of GRIWM called BRIWM, which uses \textbf{B}ootstrap resampling instead of \textbf{G}aussian resampling. Rather than resampling each fossil into the standard deviation, BRIWM uses a bootstrap technique with replacement, neglecting dating errors. This method assumes that $\theta$ can be inferred from a subsample of the original fossil time series, which may not always be true and was specifically devised to counter the assumption that all data points are equally reliable in terms of data quality. That being said, Saltr\'e et al.'s results suggest that GRIWM's ability to explicitly account for radiocarbon dating prevails over BRIWM accounting for record reliability.